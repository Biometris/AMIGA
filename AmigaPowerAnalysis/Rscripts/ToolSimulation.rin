# Import functions; before changing the working directory
source("zBinomial.rin")
source("zPoisson.rin")
require(MASS)             # For negative binomial regression 

# Retrieve parameters from call to R 
args = commandArgs(TRUE)
if (length(args) <= 1) {
  directory    = getwd()
  settingsfile = "1-Settings.csv"
  datafile     = "1-Input.csv"
  outputfile   = "1-Output.csv"
} else {
  directory    = args[1]
  settingsfile = args[2]
  datafile     = args[3]
  outputfile   = args[4]
}
setwd(directory)

# Settings
settings = read.csv(settingsfile, header=FALSE, as.is=TRUE, strip.white=TRUE)
tel = 1
ComparisonId              = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfComparisons       = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
Endpoint                  = as.character( as.vector(settings[[2]][tel]))             ; tel = tel + 1
LocLower                  = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
LocUpper                  = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
Distribution              = as.character( as.vector(settings[[2]][tel]))             ; tel = tel + 1
PowerLawPower             = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
OverallMean               = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
CVComparator              = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
CVBlocks                  = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfInteractions      = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfModifiers         = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
SignificanceLevel         = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfRatios            = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfReplications      = as.numeric(unlist(strsplit(settings[[2]][tel], " ")))    ; tel = tel + 1
ExperimentalDesignType    = as.character( as.vector(settings[[2]][tel]))             ; tel = tel + 1
PowerCalculationMethod    = as.character( as.vector(settings[[2]][tel]))             ; tel = tel + 1
RandomNumberSeed          = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
NumberOfSimulatedDataSets = as.numeric(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
IsLogNormal               = as.logical(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
IsSquareRoot              = as.logical(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
IsOverdispersedPoisson    = as.logical(   as.vector(settings[[2]][tel]))             ; tel = tel + 1
IsNegativeBinomial        = as.logical(   as.vector(settings[[2]][tel]))             ; tel = tel + 1

# Additional settings
nGCI = 10000          # Number of draws for Generalized Confidence Interval

# Define which analysis to perform
analysis = c("LogNormal", "SquareRoot", "OverdispersedPoisson", "NegativeBinomial")

# Define dispersion parameter
dispersion = ropoissonDispersion(OverallMean, CVComparator, PowerLawPower, Distribution)
dispersion 

# Define effects; always include 0 (or 1 on the original scale) 
logLocLower = log(LocLower)
logLocUpper = log(LocUpper)
effect = c(0)
if (!is.nan(LocLower)) {
  effectTmp = logLocLower + (0 - logLocLower)*rep(0:NumberOfRatios)/NumberOfRatios
  effect = c(effectTmp[1:NumberOfRatios], effect)
}
if (!is.nan(LocUpper)) {
  effectTmp = 0 + (logLocUpper - 0)*rep(0:NumberOfRatios)/NumberOfRatios
  effect = c(effect, effectTmp[1:NumberOfRatios + 1])
}
effect = exp(effect)
effect

ComparisonId
NumberOfComparisons
Endpoint

CVBlocks
ExperimentalDesignType  # CompletelyRandomized
PowerCalculationMethod  # Approximate

# Read Design matrix created by C#
data = read.csv(datafile, header=FALSE, skip=1)
ncol = ncol(data)
nrow = nrow(data)
coleffect = 2

# Rename means column and add columns for response and offsets
colnames(data)[ncol] = "mean"
data["effect"]    = NaN
data["response"]  = NaN
data["block"]     = 1
data["lowOffset"] = NaN
data["uppOffset"] = NaN
data

# Create dataframe for prediction 
preddata = head(data, 2)
preddata[,] = 0
preddata[coleffect] = c(0,1)
preddata[["block"]] = c(1,1)
preddata[["block"]] = as.factor(c(1,1))
preddata

# Create formulas for H0 and H1; Note that V2 is used to test the comparison
nameH1 = paste0("V", 3:ncol - 1)
if ((ExperimentalDesignType == "RandomizedCompleteBlocks") || (ExperimentalDesignType == "CompletelyRandomized")) {
  formulaH1 = as.formula(paste("response ~ ", paste(nameH1, collapse="+")))
} else {
  stop("ExperimentalDesignType '", ExperimentalDesignType, "' not implemented.", call. = FALSE)
}

if (ncol>4) {
    nameH0 = paste0("V", 4:ncol - 1)
    formulaH0 = as.formula(paste("response ~", paste(nameH0, collapse="+")))
    formulaH0_low = as.formula(paste("response ~", paste(nameH0, collapse="+"), " + offset(lowOffset)"))
    formulaH0_upp = as.formula(paste("response ~", paste(nameH0, collapse="+"), " + offset(uppOffset)"))
  } else {
    formulaH0 = as.formula(paste("V1 ~"))
    formulaH0_low = as.formula(paste("response ~ offset(lowOffset)"))
    formulaH0_upp = as.formula(paste("response ~ offset(uppOffset)"))
}

# Add blocking effect to model 
if (ExperimentalDesignType == "RandomizedCompleteBlocks") {
  formulaH1 = update.formula(formulaH1,  ~. + block)
  formulaH0 = update.formula(formulaH0,  ~. + block)
  formulaH0_low = update.formula(formulaH0_low,  ~. + block)
  formulaH0_upp = update.formula(formulaH0_upp,  ~. + block)
  sigBlock = sqrt(log((CVBlocks/100)*(CVBlocks/100) + 1))
} else {
  sigBlock = 0
}

# Various
set.seed(RandomNumberSeed)

# Loop over number of NumberOfReplications
nblock = length(NumberOfReplications)
neffect = length(effect)
nanalysis = length(analysis)

repBlock = rep(NumberOfReplications, each=neffect)
repEffect = rep(effect, nblock)
rownames = paste("Block=", repBlock, " Effect=", repEffect, sep="")
pvalDiff = matrix(nrow=NumberOfSimulatedDataSets, ncol=nanalysis, dimnames=list(NULL, analysis))
pvalEquiLR = matrix(nrow=NumberOfSimulatedDataSets, ncol=nanalysis, dimnames=list(NULL, analysis))
pvalEquiWD = matrix(nrow=NumberOfSimulatedDataSets, ncol=nanalysis, dimnames=list(NULL, analysis))

resultDiff = matrix(nrow=nblock*neffect, ncol=nanalysis, dimnames=list(NULL, analysis))
resultEquiLR = matrix(nrow=nblock*neffect, ncol=nanalysis, dimnames=list(NULL, analysis))
resultEquiWD = matrix(nrow=nblock*neffect, ncol=nanalysis, dimnames=list(NULL, analysis))

tel = 0
ntimes = nblock*neffect*NumberOfSimulatedDataSets
title = paste(Endpoint, "  (Endpoint ", ComparisonId, " of ", NumberOfComparisons, ")", sep="")
pb <- winProgressBar(title=title, label="Number of replications", min=0, max=ntimes, width=400)

ipb = 0
for (i in 1:nblock) {
  # Expand dataset 
  iReps = NumberOfReplications[i] 
  blocksi = iReps + 0*data[[1]]
  data.expanded = data[rep(row.names(data), blocksi), ]
  data.expanded[["block"]] = as.factor(rep(1:iReps, nrow)) 

  # Prepare for apply effect
  applyeffect = data.expanded[coleffect]==1
  data.expanded[["effect"]] = data.expanded[["mean"]]

  # Apply blocking Effect; use Blom scores
  if (ExperimentalDesignType == "RandomizedCompleteBlocks") {
    blockeff = exp(sigBlock*qnorm((1:iReps-0.375)/(iReps+0.25)))
    data.expanded[["effect"]] = data.expanded[["effect"]] * blockeff[data.expanded[["block"]]]
  }
  data.expanded["lowOffset"] = data.expanded[coleffect] * logLocLower
  data.expanded["uppOffset"] = data.expanded[coleffect] * logLocUpper
  nmean = length(data.expanded[["effect"]])

  for (j in 1:neffect) {
    data.expanded[["effect"]][applyeffect] = effect[j]*data.expanded["mean"][applyeffect]
    # Do looping over simulations 
    for (k in 1:NumberOfSimulatedDataSets) {
      simulated = ropoisson(nmean, data.expanded[["effect"]], dispersion, PowerLawPower, Distribution)
      # ----------------------------------------------------------------------------------------------
      # OverdispersedPoisson -------------------------------------------------------------------------
      # ----------------------------------------------------------------------------------------------
      if (IsOverdispersedPoisson) {
        data.expanded[["response"]] = simulated
        glmH0 = glm(formulaH0, family="quasipoisson", data=data.expanded, mustart=data.expanded[["effect"]])
        glmH1 = glm(formulaH1, family="quasipoisson", data=data.expanded, mustart=data.expanded[["effect"]])
        # Difference test         esti = glmH1$coef[2]
        df1 = df.residual(glmH1)
        estDispersion = sum(residuals(glmH1,type="pearson")^2)/df1
        pval = pf((deviance(glmH0) - deviance(glmH1))/estDispersion, 1, df1, lower.tail=FALSE)
        pvalDiff[k, "OverdispersedPoisson"] = pval
        if (k == 1) {
          qt = abs(qt(SignificanceLevel/2, df1, lower.tail=TRUE))
        }
        # Equivalence test 
        estiEffect = glmH1$coef[2]
        if ((estiEffect < logLocLower) | (estiEffect > logLocUpper)) {
          pvalEquiWD[k, "OverdispersedPoisson"] = 1
          pvalEquiLR[k, "OverdispersedPoisson"] = 1
        } else {
          # Wald equivalence test
          seEffect = sqrt(vcov(glmH1)[2,2])
          lower = estiEffect - qt * seEffect
          upper = estiEffect + qt * seEffect
          if ((lower>logLocLower) && (upper<logLocUpper)) {
            pvalEquiWD[k, "OverdispersedPoisson"] = 0
          } else {
            pvalEquiWD[k, "OverdispersedPoisson"] = 1
          }
          # LR equivalence test
          glmH0low = glm(formulaH0_low, family="quasipoisson", data=data.expanded, mustart=fitted.values(glmH0))
          glmH0upp = glm(formulaH0_upp, family="quasipoisson", data=data.expanded, mustart=fitted.values(glmH0))
          pvalLow = pf((deviance(glmH0low) - deviance(glmH1))/estDispersion, 1, df1, lower.tail=FALSE)
          pvalUpp = pf((deviance(glmH0upp) - deviance(glmH1))/estDispersion, 1, df1, lower.tail=FALSE)
          pvalEquiLR[k, "OverdispersedPoisson"] = max(pvalLow, pvalUpp)
        } 
      }
      #print("OverdispersedPoisson completed")
      # ----------------------------------------------------------------------------------------------
      # LogNormal ------------------------------------------------------------------------------------
      # ----------------------------------------------------------------------------------------------
      if (IsLogNormal) {
        data.expanded[["response"]] = log(simulated + 1)
        lmH1 = lm(formulaH1, data=data.expanded)
        pval = 2*pt(abs(lmH1$coef[2])/sqrt(vcov(lmH1)[2,2]), lmH1$df.residual, lower.tail=FALSE)
        pvalDiff[k, "LogNormal"] = pval
        if (FALSE) {

      print(summary(lmH1))
      print(deviance(lmH1))
      print(df.residual(lmH1))
      print("LogNormal Fitting completed")
        # Equivalence testing by means of GCI
        predict = predict(lmH1, preddata, se=TRUE)
        print(predict)
        effGMO = predict$fit[1]
        effCMP = predict$fit[2]
        effReplication = deviance(lmH1)/df.residual(lmH1)/(predict$se.fit^2)
        print(effReplication)
        print(effGMO)
        stop("stop")
        }


      }

      # ----------------------------------------------------------------------------------------------
      # SquareRoot -----------------------------------------------------------------------------------
      # ----------------------------------------------------------------------------------------------
      if (IsSquareRoot %in% analysis) {
        data.expanded[["response"]] = sqrt(simulated)
        lmH1 = lm(formulaH1, data=data.expanded)
        pval = 2*pt(abs(lmH1$coef[2])/sqrt(vcov(lmH1)[2,2]), lmH1$df.residual, lower.tail=FALSE)
        pvalDiff[k, "SquareRoot"] = pval
      }
      # ----------------------------------------------------------------------------------------------
      # NegativeBinomial -----------------------------------------------------------------------------
      # ----------------------------------------------------------------------------------------------
      if (IsNegativeBinomial) {
        data.expanded[["response"]] = simulated
        glmH0 = glm.nb(formulaH0, data=data.expanded, link=log)
        glmH1 = glm.nb(formulaH1, data=data.expanded, link=log, mustart=fitted.values(glmH0))
        pval = pchisq(-2*(logLik(glmH0) - logLik(glmH1)), 1, lower.tail=FALSE)
        pvalDiff[k, "NegativeBinomial"] = pval

        estiEffect = glmH1$coef[2]
        if ((estiEffect < logLocLower) | (estiEffect > logLocUpper)) {
          pval = 1
        } else {
          glmH0low = glm.nb(formulaH0_low, data=data.expanded, link=log, mustart=fitted.values(glmH0))
          glmH0upp = glm.nb(formulaH0_upp, data=data.expanded, link=log, mustart=fitted.values(glmH0))
          pvalLow = pchisq(deviance(glmH0low) - deviance(glmH1), 1, lower.tail=FALSE)
          pvalUpp = pchisq(deviance(glmH0upp) - deviance(glmH1), 1, lower.tail=FALSE)
          pval = max(pvalLow, pvalUpp)
        } 
        pvalEquiLR[k, "NegativeBinomial"] = pval
      }
      ipb = ipb + 1
      setWinProgressBar(pb, ipb, label=sprintf("Number of replications: %s", iReps))
    }
    tel = tel + 1
    resultDiff[tel,] = colSums(pvalDiff < SignificanceLevel)/NumberOfSimulatedDataSets
    resultEquiLR[tel,] = colSums(pvalEquiLR < SignificanceLevel)/NumberOfSimulatedDataSets
    resultEquiWD[tel,] = colSums(pvalEquiWD < SignificanceLevel)/NumberOfSimulatedDataSets
  }
}
warnings()
print(resultDiff)
print(resultEquiLR)
print(resultEquiWD)

# Combine results into a single dataframe
df=cbind(repEffect, repEffect, repBlock, resultDiff, resultEquiLR, resultEquiWD)
df[,2] = log(df[,2])
dfnames = c("Ratio",  "LogRatio",	"NumberReplicates", 
    paste("PowerDifference", analysis, sep=""), 
    paste("PowerEquivalenceLR", analysis, sep=""), 
    paste("PowerEquivalenceWD", analysis, sep=""))
colnames(df) = dfnames

# Sort according to Ratio and NumberReplicates
df = df[ order(df[,2], df[,3]), ]


write.table(df, file=outputfile, quote=FALSE, sep=",", row.names=FALSE, col.names=TRUE, na="*")
q()

